# 1、强化学习概述
## 1.1 流程
在强化学习框架中，一般拥有两个实体，智能体Agent和环境Environment，强化学习中两个实体的交互中有几个概念：状态空间S：指环境中所有可能状态的集合；动作空间A：指智能体所有可能动作的集合；奖励R:指智能体在环境的某一状态下所获得的奖励   

  一般情况下，智能体与环境的交互过程如下：  
  · 在t时刻，环境的状态为St，达到这一状态所获得的奖励为Rt  
  · 智能体观测到St和Rt后，采取相应的动作At  
  · 智能体采取At后，环境状态变为St+1，得到的奖励为Rt+1  

  智能体在这个过程中学习，最终目标是：**找到一个策略，这个策略根据当前观测的环境状态和奖励反馈来选择最佳的动作**
## 1.2 价值函数  
在强化学习中，价值函数衡量的是**从某个状态（或状态-动作）出发，未来能获得的“预期总回报”有多大**  

  具体而言 t时刻状态s的总收益 = 身处状态s能带来的即时收益 + 从状态s出发后能带来的未来收益  
  写成表达式为：Vt = Rt + βVt+1  
  其中Vt代表t时刻的总收益，Rt代表t时刻即时收益，Vt+1代表t+1时刻的总收益，β代表折扣因子，决定在多大程度上考虑将“未来收益”纳入到“当下收益”  
# 2、在NLP中如何理解上述角色定义 
在NLP中，t时刻，模型根据上文产生一个token，这个token即对应着强化学习中的动作，记为At，此时模型产出token At对应着的即时收益为Rt（按1.1应为Rt+1，为方便记为Rt），总收益为Vt，此时模型的状态由St变为St+1，也就是从“上文”变成“上文+新产出的token”  
# 3、PPO中的四个模型
## 3.1 Actor Model
即要训练的目标语言模型，一般用SFT阶段产出的SFT模型来初始化。最终目的是让Actor模型产生能符合人类喜欢的response。  
策略为喂给Actor一条prompt，生成对应的response，然后将“prompt+response”送入奖励-loss体系计算loss更新Actor模型  
## 3.2 Reference Model
一般也用SFT阶段得到的SFT模型做初始化，训练过程中，参数冻结，作用是防止模型训歪，具体方法是使用KL散度，具体公式见下图：
<div align=center>
  <img src="https://github.com/user-attachments/assets/2f26803b-d508-4a6e-a1a1-67ae65e53fb9" width="500" />
</div>
KL散度衡量的是从Q走到P要花多少额外的信息代价，DKL(P||Q) != DKL(Q||P)，并且非负仅当两者相同时等号成立。  

对于Actor Model输出response时，对每个生成的token都会有对应的概率，同样的对于Ref模型，将Actor生成的prompt+response也喂给它，同样能给出每个token的prob结果    
                你好
