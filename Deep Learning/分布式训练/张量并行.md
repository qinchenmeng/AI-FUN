# 张量并行
## 1.为什么需要张量并行  
当模型参数太大，一个GPU装不下时，就不能简单使用数据并行，而是需要把模型的结构拆开放到多个GPU上，这就是模型并行的做法，张量并行是其中的主要做法。

## 2.切分权重
首先我们知道神经网络每次前向传播时的流程如下：Y = X*W，其中Y的维度为（b,s,h'），X的维度为（b,s,h），W的维度为（h，h'），b为batch_size，s为输入序列的长度，h为每个token向量的隐维度，h'为参数矩阵W的隐藏层维度。  
现在W太大，导致单卡装不下，需要将W切开放不到不同的卡上，则现在面临三个问题：
- 如何切分W
- 切完w后，如何forward
- forward后，如何backward

一般情况下，我们可以沿着W的行（h维度）或者W的列（h'维度）切分W  
### 2.1 按行切分
