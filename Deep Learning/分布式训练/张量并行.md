# 张量并行
## 1.为什么需要张量并行  
当模型参数太大，一个GPU装不下时，就不能简单使用数据并行，而是需要把模型的结构拆开放到多个GPU上，这就是模型并行的做法，张量并行是其中的主要做法。

## 2.切分权重
首先我们知道神经网络每次前向传播时的流程如下：Y = X*W，其中Y的维度为（b,s,h'），X的维度为（b,s,h），W的维度为（h，h'），b为batch_size，s为输入序列的长度，h为每个token向量的隐维度，h'为参数矩阵W的隐藏层维度。  
现在W太大，导致单卡装不下，需要将W切开放不到不同的卡上，则现在面临三个问题：
- 如何切分W
- 切完w后，如何forward
- forward后，如何backward

一般情况下，我们可以沿着W的行（h维度）或者W的列（h'维度）切分W  
### 2.1 按行切分【求和】
（1）forward  
即将W按行维度切成GPU数量个数的分数，如N，下图N=2  
<div align=center>
  <img src="https://github.com/user-attachments/assets/f90facbd-82ed-4f4a-b01a-42d1c08f2fc2" width="500" />
</div>
如果W按照行维度切开后，X的维度就和它不对齐， 如何做矩阵乘法，做法即将X按照列拆开，如下图所示，【可以简单推算下，下图矩阵乘法完全成立】
<div align=center>
  <img src="https://github.com/user-attachments/assets/41019151-c9ac-4d44-9ee4-2b6fbe40579d" width="500" />
</div>
（2）backward  
做完forward，取得预测值Y,进而可计算出损失L，接下来就能做backward了，如下图所示  
<div align=center>
  <img src="https://github.com/user-attachments/assets/b4933066-c856-4502-b7c3-22a64e80ef4c" width="500" />
</div>
  
  图中f和g表示两个算子，其实就是改变隐向量维度的算子，反向传播在每一次主要需要计算两个梯度，对权重的梯度和对输入的梯度，后者主要是为了继续传播到前面的层  
g的backward代表的就是对wi计算梯度，根据右下角的公式，可以知道只需要将【L/Y】的梯度广播到两块GPU上，两块GPU就可以独立计算各自权重的梯度了。【这里在广播时，是将梯度按列拆分为两份分别广播，因为多余的梯度没用】  
f的backward代表的，当模型存在多层时，梯度需要多层传播，也可以推出左下角的公式，这里注意是两者应该相加，而不是拼接
### 2.2 按列切分权重【拼接】
（1）forward  
即将W按列维度切成GPU数量个数的分数，如N，下图N=2
<div align=center>
  <img src="https://github.com/user-attachments/assets/ab47807b-2ff6-4723-927c-b6a3b15f6e5b" width="500" />
</div>

(2)backward  
见下图
<div align=center>
  <img src="https://github.com/user-attachments/assets/11c7c5ba-fa32-4054-88a9-2a38175f6a2c" width="500" />
</div>
其中，f的backward时，因为X参与了两个xw1和xw2的计算，所以计算梯度时要两者相加


## 3.MLP层  
### 3.1 MLP层的张量并行计算方法
<div align=center>
  <img src="https://github.com/user-attachments/assets/df827d5c-34b5-432a-a37e-43d1499ef0d2" width="500" />
</div>
假设现在有N块GPU，现在要把MLP层的权重拆到上面做计算，Megatron提供的切分方法如下图：
<div align=center>
  <img src="https://github.com/user-attachments/assets/66662d6a-9f29-4df9-a406-af8d0a55d8a3" width="800" />
</div>

具体的操作方法为，对A采用“列切割”，对B采用“行切割”
<pre> AllReduce 是分布式计算中常用的一种通信操作，它的作用是：对多个设备（如多张 GPU）上的数据做 “归约”（如求和）操作【Reduce-Scatter】，并将结果广播到所有设备上【All-Gather】。</pre>  
为什么对A进行列切割，对B采取行切割呢？    
首先gelu激活函数是逐元素的非线形函数，不能与加法操作交换，即GELU[X]= GELU[X1+X2] != GELU[X1] + GELU[X2]，而根据按行切分的定义我们可以知道，按行切分最后是一个加和【非拼接】的操作，所以对非线形时，只能按列切分并拼接。当A进行列切割后，B也只能进行行切割了。  
### 3.2 MLP层的通讯量分析  
由上图分析，MLP层做forward时产生一次AllReduce，做backward时产生一次AllReduce。Allreduce分为两个阶段，Reduce-Scatter和All-Gather，每个阶段的通讯量都相等，现在假设每个阶段的通讯量为Z，那么一次AllReduce产生的通讯量为2Z，MLP层的总通讯量为4Z，其中Z=b×s×h。  
## 4.Self-Attention层  
### 4.1 Multi-head Attention的计算方式  
在多头的情况下，下图展示了当num_heads=2时，attention层的计算方法，即对每一块权重，我们都沿着列方向（k_dim）维度切割一刀，此时每个head上的W^q,W^k,W^v的维度都变成【d_model,k_dim//2】，每个head上单独做矩阵计算，最后将计算结果concat起来即可。整个流程如下：
<div align=center>
  <img src="https://github.com/user-attachments/assets/7b0bcce2-d405-4283-ab40-a8d786c4c93b" width="800" />
</div>
根据上图可以发现，attention的多头计算就说为张量并行量身定做的，因为每个头都可以独立计算，最后再将结果concat起来，也就是说每个头的参数可以放到一块GPU上，整个过程如下图：
<div align=center>
  <img src="https://github.com/user-attachments/assets/0e329298-63f5-4903-b43a-6dfcb20268f8" width="800" />
</div>
对三个参数矩阵Q,K,V，按照“列切割”，每个头放在一在GPU上，做并行计算，对线性层B，按照“行切割”，最后相加即可。    

### 4.2 Self-Attention层的通讯量分析  
类比于MLP层，self-attention层在forward中做一次AllReduce，在backward时做一次AllReduce，总通讯量也是4Z，其中Z=b×s×h。 
## 5.Embedding层
### 5.1 输入层Embedding
输入层的Embedding层一般有两个部分组成：**word embedding**，维度（v，h），其中v表示词表大小；**postional embedding**，维度（max_s,h），其中max_s表示模型允许的最大序列长度。  
对于positional embedding来说，max_s本身不会太长，因此每个GPU上都拷贝一份，对显存的压力也不会太大，但是对于word embedding来说，词表的大小很客观，因此需要把word embedding拆分到各个GPU上，具体做法如下：
<div align=center>
  <img src="https://github.com/user-attachments/assets/ac62e156-49b7-425d-b1c2-c8b7c6bc9b30" width="800" />
</div>
上图中，对于输入X，过word embedding的过程，就是等于用token的序号去word embedding中查找对应词向量的过程，例如输入数据的token序号为[0,212,7,9],我们要做的就是去word embedding中的0,212,7,9行去把相应的词向量找出来。  

  假设词表中有300个词，现在我们将word embedding拆分到两块GPU上，第一块GPU维护词表[0, 150)，第二块GPU维护词表[150, 299)。当输入X去GPU上查找时，能找到的词，就正常返回词向量，找到不到就把词向量中的全部全素都置0。按此方式查找完毕后，每块GPU上的数据做一次AllReduce，就能得到最终的输入
### 5.2 输出层Embedding  
输出层中，同样有一个word embedding，把输入映射为词表，得到每一个位置的词的分数。在transformer中，输出层的embedding表现为Linear层+softmax，来从隐藏状态生成最终的token概率分布。这个线性层W的参数，常和输入的word embedding共享【权重共享】，即输出层的权重矩阵等于输入的embedding矩阵的转置，这样可以降低模型参数量，实验验证效果更好，经过linear层后得到一个长度为V，即词表大小的向量，此时为原始打分，没有经过归一化，通过softmax将这些logits变成一个概率分布，这样才能在训练中通过交叉熵损失比较预测概率与真实标签的差距，推理时选择概率最大的词作为输出。计算过程如下：
<div align=center>
  <img src="https://github.com/user-attachments/assets/9e0079da-339b-48a0-a259-a378e0bc501e" width="800" />
</div>
需要注意的是，时刻保证输入层和输出层共用一套word embedding，而在backward是，在输出层对word embedding计算一次梯度，在输入层还会对word embedding计算一次梯度，在用梯度做word embedding权重 更新时，必须保证用两次梯度的总和进行更新。  
当模型的输入层到输入层都在一块GPU上时（即流水线并行深度=1），我们不必担心这点（实践中大部分用Megatron做并行的项目也是这么做的）。但若模型输入层和输出层在不同的GPU上时，我们就要保证在权重更新前，两块GPU上的word embedding梯度做了一次AllReduce

## 6.Cross-entropy层
输出层过完embedding后的样子如下图：
<div align=center>
  <img src="https://github.com/user-attachments/assets/d1d8ea40-99b7-4001-8926-0af61124d147" width="800" />
</div>
正常来说，需要对Y1和Y2做一次All-Gather，concat起来后形成Y，然后对Y的每一行做softmax，就可得到对于每个位置来说，每个词出现的概率，接着在用此概率和真值组做cross-entropy即可。但是All-Gather回产生额外的通讯量b×s×v，当词表v很大时，这个通讯开销也不容忽视.针对这种情况，可以做如下优化：  

<div align=center>
  <img src="https://github.com/user-attachments/assets/307bdc5f-c949-4483-992d-3402d91cec2f" width="800" />
</div>

· 每块GPU上，我们可以先按行求和，得到各自GPU上的GPU_SUM(e)  
· 将每块GPU上结果做ALLReduce，得到每行最终的SUM(e)，也就是softmax中的分母，此时的通讯量为b×s  
· 在每块GPU上，即可计算各自维护部分的 e/SUM(e),将其与真值做cross-entropy，得到每行的loss，按行加总起来得到GPU上scalar loss【即一个标量，每行loss加和】  
· 将GPU上的scalar loss做AllReduce，得到总Loss，此时通讯量为N  
这样，我们把原先的通讯量从b×s×v，大大降至b×s+N
