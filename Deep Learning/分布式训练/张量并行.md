# 张量并行
## 1.为什么需要张量并行  
当模型参数太大，一个GPU装不下时，就不能简单使用数据并行，而是需要把模型的结构拆开放到多个GPU上，这就是模型并行的做法，张量并行是其中的主要做法。

## 2.切分权重
首先我们知道神经网络每次前向传播时的流程如下：Y = X*W，其中Y的维度为（b,s,h'），X的维度为（b,s,h），W的维度为（h，h'），b为batch_size，s为输入序列的长度，h为每个token向量的隐维度，h'为参数矩阵W的隐藏层维度。  
现在W太大，导致单卡装不下，需要将W切开放不到不同的卡上，则现在面临三个问题：
- 如何切分W
- 切完w后，如何forward
- forward后，如何backward

一般情况下，我们可以沿着W的行（h维度）或者W的列（h'维度）切分W  
### 2.1 按行切分
（1）forward  
即将W按行维度切成GPU数量个数的分数，如N，下图N=2  
<div align=center>
  <img src="https://github.com/user-attachments/assets/f90facbd-82ed-4f4a-b01a-42d1c08f2fc2" width="500" />
</div>
如果W按照行维度切开后，X的维度就和它不对齐， 如何做矩阵乘法，做法即将X按照列拆开，如下图所示，【可以简单推算下，下图矩阵乘法完全成立】
<div align=center>
  <img src="https://github.com/user-attachments/assets/41019151-c9ac-4d44-9ee4-2b6fbe40579d" width="500" />
</div>
（2）backward  
做完forward，取得预测值Y,进而可计算出损失L，接下来就能做backward了，如下图所示  
<div align=center>
  <img src="https://github.com/user-attachments/assets/b4933066-c856-4502-b7c3-22a64e80ef4c" width="500" />
</div>
  
  图中f和g表示两个算子，其实就是改变隐向量维度的算子，反向传播在每一次主要需要计算两个梯度，对权重的梯度和对输入的梯度，后者主要是为了继续传播到前面的层  
g的backward代表的就是对wi计算梯度，根据右下角的公式，可以知道只需要将【L/Y】的梯度广播到两块GPU上，两块GPU就可以独立计算各自权重的梯度了。【这里在广播时，是将梯度按列拆分为两份分别广播，因为多余的梯度没用】  
f的backward代表的，当模型存在多层时，梯度需要多层传播，也可以推出左下角的公式，这里注意是两者应该相加，而不是拼接
### 2.2 按列切分权重
（1）forward  
即将W按列维度切成GPU数量个数的分数，如N，下图N=2
<div align=center>
  <img src="https://github.com/user-attachments/assets/ab47807b-2ff6-4723-927c-b6a3b15f6e5b" width="500" />
</div>

(2)backward  
见下图
<div align=center>
  <img src="https://github.com/user-attachments/assets/11c7c5ba-fa32-4054-88a9-2a38175f6a2c" width="500" />
</div>
其中，f的backward时，因为X参与了两个xw1和xw2的计算，所以计算梯度时要两者相加


## 3.MLP层  
### 3.1 MLP层的张量并行计算方法
<div align=center>
  <img src="https://github.com/user-attachments/assets/df827d5c-34b5-432a-a37e-43d1499ef0d2" width="500" />
</div>
假设现在有N块GPU，现在要把MLP层的权重拆到上面做计算，Megatron提供的切分方法如下图：
<div align=center>
  <img src="https://github.com/user-attachments/assets/66662d6a-9f29-4df9-a406-af8d0a55d8a3" width="800" />
</div>
