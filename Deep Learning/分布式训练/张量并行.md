# 张量并行
## 1.为什么需要张量并行  
当模型参数太大，一个GPU装不下时，就不能简单使用数据并行，而是需要把模型的结构拆开放到多个GPU上，这就是模型并行的做法，张量并行是其中的主要做法。

## 2.切分权重
首先我们知道神经网络每次前向传播时的流程如下：Y = X*W，其中Y的维度为（b,s,h'），X的维度为（b,s,h），W的维度为（h，h'），b为batch_size，s为输入序列的长度，h为每个token向量的隐维度，h'为参数矩阵W的隐藏层维度。  
现在W太大，导致单卡装不下，需要将W切开放不到不同的卡上，则现在面临三个问题：
- 如何切分W
- 切完w后，如何forward
- forward后，如何backward

一般情况下，我们可以沿着W的行（h维度）或者W的列（h'维度）切分W  
### 2.1 按行切分
即将W按行维度切成GPU数量个数的分数，如N，下图N=2  
<div align=center>
  <img src="https://github.com/user-attachments/assets/f90facbd-82ed-4f4a-b01a-42d1c08f2fc2" width="500" />
</div>
如果W按照行维度切开后，X的维度就和它不对齐， 如何做矩阵乘法，做法即将X按照列拆开，如下图所示，【可以简单推算下，下图矩阵乘法完全成立】
<div align=center>
  <img src="https://github.com/user-attachments/assets/41019151-c9ac-4d44-9ee4-2b6fbe40579d" width="500" />
</div>
