# 流水线并行  
## 1.优化目标  
分布式训练的总体目标是什么呢？  
· 能训练更大的模型  
· 能更快的训练模型  
## 2.模型并行
当一个单卡装不下的大模型时，最直接的解决办法是，把模型隔成不同的层，从而放到不同的GPU上  
<div align=center>
  <img src="https://github.com/user-attachments/assets/0d696de0-9f84-4e6f-b5b6-02c8ba256063" width="500" />
</div>
此时，模型做一轮前向传播和反向传播的过程如下：
<div align=center>
  <img src="https://github.com/user-attachments/assets/ddf671e1-6642-46a1-bbf3-e41943466b3d" width="500" />
</div>
其中下标表示batch编号，这里只有一个batch，因此下标为0，每一行表示一个GPU，每一列表示一个时间步  
这张图的含义是：我在GPU0上做完一次forward，然后将GPU0上最后一层的输入传给GPU1，继续做forward，直到四块GPU都做完forward后，我再依次做backward。等把四块GPU上的backward全部做完后，最后一个时刻我统一更新每一层的梯度。  

  这样的做法带来两个问题：（1）GPU利用度不够，且GPU越多时，空置的比例接近1，即GPU的资源都被浪费掉了。（2）中间结果占据大量内存，反向传播计算梯度时，由于需要用到每一层的中间结果，所以占用大量内存  

## 3.流水线并行  
（1）micro-batch  
核心思想是在模型并行的基础上，进一步引入数据并行的办法，即将原先的数据，再划分为若干个batch，送入gpu训练，即在batch上再划分的数据，叫micro-batch
<div align=center>
  <img src="https://github.com/user-attachments/assets/b5f131bd-7c22-4cf6-9488-763d99d3e94b" width="500" />
</div>
其中，第一个下标表示GPU编号，第二个下标表示micro-batch编号。  
   
   （2）re-materialization  
   解决了GPU的空置问题，提高了GPU计算的整体效率，接下来就要解决GPU的内存问题。对此，Gpipe(谷歌推出)的做法是**用时间换空间**，也就是几乎不存中间结果，得到回传的时候，再forward计算一遍  
<div align=center>
  <img src="https://github.com/user-attachments/assets/f53457cd-059a-45de-bcab-3a708a398f3b" width="500" />
</div>  
每块GPU上，我们只保存来自上一块的最后一层输入z，其余的中间结果我们算完就废。等到backward的时候再由保存下来的z重新进行forward来算出。  
那么空间复杂度优化了多少呢（这里不考虑模型自身参数，只在考虑算法优化）：  
  
  设原本batchsize为B，模型共L层，K个GPU，d为神经元个数（激活函数是神经元区别于简单线性计算的核心标志），每一层有d个神经元即d个激活值。原始模型并行中，对于每块GPU，中间存储参数量所占用的空间为O[B*(L/K)*d]。随着模型增大，B,L,d，可能会平滑掉K增加带来的内存收益    
  那么经过re-materizaliaiton后，因为用完就废，那么每块GPU峰值时刻的空间复杂度为O[B*d+(B/M)*(L/K)*d]，其中M为micro-batch大小，B*d为每个数据只保存最后一层的激活值
