# 流水线并行  
## 1.优化目标  
分布式训练的总体目标是什么呢？  
· 能训练更大的模型  
· 能更快的训练模型  
## 2.模型并行
当一个单卡装不下的大模型时，最直接的解决办法是，把模型隔成不同的层，从而放到不同的GPU上  
<div align=center>
  <img src="https://github.com/user-attachments/assets/0d696de0-9f84-4e6f-b5b6-02c8ba256063" width="500" />
</div>
此时，模型做一轮前向传播和反向传播的过程如下：
<div align=center>
  <img src="https://github.com/user-attachments/assets/ddf671e1-6642-46a1-bbf3-e41943466b3d" width="500" />
</div>
其中下标表示batch编号，这里只有一个batch，因此下标为0，每一行表示一个GPU，每一列表示一个时间步  
这张图的含义是：我在GPU0上做完一次forward，然后将GPU0上最后一层的输入传给GPU1，继续做forward，直到四块GPU都做完forward后，我再依次做backward。等把四块GPU上的backward全部做完后，最后一个时刻我统一更新每一层的梯度。  

  这样的做法带来两个问题：（1）GPU利用度不够，且GPU越多时，空置的比例接近1，即GPU的资源都被浪费掉了。（2）中间结果占据大量内存，反向传播计算梯度时，由于需要用到每一层的中间结果，所以占用大量内存  

## 3.流水线并行  
（1）micro-batch  
核心思想是在模型并行的基础上，进一步引入数据并行的办法，即将原先的数据，再划分为若干个batch，送入gpu训练，即在batch上再划分的数据，叫micro-batch
<div align=center>
  <img src="https://github.com/user-attachments/assets/b5f131bd-7c22-4cf6-9488-763d99d3e94b" width="500" />
</div>
其中，第一个下标表示GPU编号，第二个下标表示micro-batch编号。  
   
   （2）re-materialization
