# 数据并行
## 一、数据并行（DP）
### 1.1 整体架构
<div align=center>
  <img src="https://github.com/user-attachments/assets/d8e586fc-baa0-4438-a224-8e750a32696b" width="800" />
</div>

一个经典数据并行的过程如下：  
· 若干块计算GPU，如上图中GPU0-GPU2，1块梯度收集GPU，如图中AllReduce操作所在GPU  
· 在每块计算GPU上都拷贝一份完整的模型参数  
· 把一份数据X【例如一个batch】均匀分给不同的计算GPU  
· 每块计算GPU做一轮FWD和BWD后，算得一份梯度G  
· 每块计算GPU将自己的梯度push给梯度收集GPU，做聚合操作。这里的聚合操作一般指**梯度累加**  
· 梯度收集GPU聚合完毕后，计算GPU从它那pull下完整的梯度效果，用于更新模型参数W，更新完毕后，计算GPU上的模型参数应该保持一致  

  实现DP的一种经典编程框架叫做“参数服务器”，在这个框架里，计算GPU成为Worker，梯度聚合GPU称为Server。在实际应用中，为了尽量减少通讯量，一般可选择一个Worker同时作为Server，比如可把梯度全发到GPU0上做聚合，这里需要说明两点：  
  · 一个worker或者server下可以不止一块GPU  
  · Server可以只做梯度聚合，也可以梯度聚合+全量参数更新一起做  
### 1.2 存在问题：通讯瓶颈和梯度异步更新
实战中存在两个问题：  
· **存储开销大**：每个GPU上都存了一份完整的模型，造成冗余。  
· **通讯开销大**：Server需要和每一个worker进行梯度传输，当Server和worker不在一台机器上时，Server的带宽将会成为整个系统的计算效率瓶颈  
当Server在搬运数据，计算梯度时，worker闲置，针对这种情况，提出了梯度异步更新策略：
**梯度异步更新**  
计算顺序如下：  
· 在第n轮计算中，该worker正常计算梯度，并向server发送push&pull梯度请求。  
· 但是，该worker不会实际等到把聚合梯度拿回来，更新完参数W后再做计算，而是直接拿旧的W，吃新的数据，继续第11轮的计算，这样就保证在通讯的时间里，worker也在马不停蹄的计算，提升计算通讯比。  
· 当然，异步也不能只计算梯度，不更新权重，那模型无法收敛，可以存在**延迟**，比如延迟为1的异步更新就是当开始第12轮对的计算时，必须保证W已经用第10、11轮的梯度做完2次更新了。  
总结一下，异步对一个worker来说，只是等于W不变，batch的数量增加了而已，在SGD下，会减慢模型的整体收敛速度，异步的整体思想是，比起让Worker闲着，倒不如让它多吃点数据，虽然反馈延迟了，但只要它在干活在学习就行。
## 二、分布式数据并行（DDP）
受通讯负载不均的影响，DP一般用于单机多卡场景，而DDP既能多机也能单机。DDP主要解决的问题就是通讯问题，将Server的通讯压力均衡到各个worker上。AllReduce的操作为聚合梯度+下发梯度。接下来介绍更通用的AllReduce方法：Ring-AllReduce，解决的就是数据并行中通讯负载不均的问题，使得DDP得以实现  
### 2.1 Ring-AllReduce
如下图所示，假设有4块GPU，每块GPU上的数据也对应被切分成4份吗，AllReduce的最终目标就是让每块GPU上的数据都变成箭头右边汇总的样子
<div align=center>
  <img src="https://github.com/user-attachments/assets/9b4710f6-fa91-454e-8349-2629240efb6c" width="800" />
</div>
Ring-AllReduce分两大步骤实现该目标：Reduce-Scatter和All-Gather

**Reduce-Scatter**  
定义网络拓扑关系，使得每个GPU只和相邻的两块GPU通讯，每次发送对应位置的数据【梯度】进行累加，每一次累加更新都形成一个拓扑环，因此被成为Ring。步骤如下图
![image](https://github.com/user-attachments/assets/803957a9-fb26-42b2-a346-3aab7ac8d08a)
![image](https://github.com/user-attachments/assets/3a718e3f-16b4-440f-8d48-f65437e2a276)
一次累加完毕后，蓝色位置的数据【梯度】块被更新，被更新的数据【梯度】块将成为下一次更新的起点，继续做累加操作。
![image](https://github.com/user-attachments/assets/46a455b6-1723-47b3-bec7-53544fb6480b)
![image](https://github.com/user-attachments/assets/6f3dd62a-fbde-462e-bf08-eaf05f03c96f)
可以发现，3次更新之后。每块GPU上都有了一块数据拥有了对应位置完整的聚合，此时Reduce-Scatter阶段结束，进入All-Gather阶段，目的就是把红色快的数据【梯度】广播道其余GPU对应的位置上。  
**All-Gather**  
这里的操作依然按照“相邻GPU对应位置进行通讯”的原则，但对应位置数据不再做相加，而是直接替换。如下图
![image](https://github.com/user-attachments/assets/28a612e0-d07c-4786-82a3-519e6e745659)
![image](https://github.com/user-attachments/assets/fa95a28a-9ef1-46c7-aca0-864438394771)
以此类推，经过3轮迭代后，每块GPU上都汇总到了完整的数据，变成如下形式
<div align=center>
  <img src="https://github.com/user-attachments/assets/7f812be1-85ed-4b77-beff-f7971952fe74" width="500" />
</div>

### 2.2 Ring-AllReduce通讯量分析
假设模型参数W大小为Z，GPU个数为N，则梯度大小也为Z，每个梯度块的大小为 Z/N。  
对单卡GPU来说，Reduce-Scatter阶段的通讯量为(N-1)×Z/N【这里不要考虑图中分的各个GPU的小块，虽然每次发送的是多个梯度，但是这里是梯度的聚合，已经加和过后的了，所以发送的数据【梯度】量一直没变，一直是一份数据【梯度】】。All-Gather阶段，通讯量为(N-1)×Z/N。  
单卡总通讯量为 2(N-1)×Z/N，随着N的增大，可以近似为2Z【单卡通讯量】，全卡总通讯量为2NZ。  

  而对于DP来说，它的Server承载的通讯量为NZ，所有Worker承载的总通讯量为NZ【计算通讯时间主要看单卡】，全卡总通讯量依然为 2NZ，虽然通讯量相同，但搬运相同数据【梯度】量的时间却不相同。在DDP中，通讯量均衡负载到了每一个时刻的worker上，而DP却只让Server进行搬运，当GPU越来越多时，DP的通讯时间会直线上升。  
  **问题一**：DDP中单卡GPU梯度为什么要被切分？  
  其实也可以不切分，那么所有梯度完成互相之间的总通讯量为  (N-1)*Z【为什么不需要×2，因为是环形】 ，这与DP的差距不是很大。这时将梯度分为份，每次传输的梯度量为 Z/N，且经过特殊的设计，总过通讯 2(N-1)次也可达到目的，整体时间计算抵消掉N变得再也不跟N线性相关了。

## 三、总结  
1、在DP中，每个GPU上都拷贝一份完整的模型，每个GPU上处理batch的一部分数据，所有GPU算出来的梯度进行累加后，再传回各GPU用于更新参数  
2、DP多采用参数服务器这一编程框架，一般由若个计算Worker和1个梯度聚合Server组成。Server与每个Worker通讯，Worker间并不通讯。因此Server承担了系统所有的通讯压力。基于此DP常用于单机多卡场景。  
3、异步梯度更新是提升计算通讯比的一种方法，延迟更新的步数大小决定了模型的收敛速度。  
4、Ring-AllReduce通过定义网络环拓扑的方式，将通讯压力均衡地分到每个GPU上，使得跨机器的数据并行（DDP）得以高效实现。  
5、DP和DDP的总通讯量相同，但因负载不均的原因，DP需要耗费更多的时间搬运数据
