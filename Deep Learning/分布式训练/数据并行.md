# 数据并行
## 一、数据并行（DP）
### 1.1 整体架构
<div align=center>
  <img src="https://github.com/user-attachments/assets/d8e586fc-baa0-4438-a224-8e750a32696b" width="800" />
</div>

一个经典数据并行的过程如下：  
· 若干块计算GPU，如上图中GPU0-GPU2，1块梯度收集GPU，如图中AllReduce操作所在GPU  
· 在每块计算GPU上都拷贝一份完整的模型参数  
· 把一份数据X【例如一个batch】均匀分给不同的计算GPU  
· 每块计算GPU做一轮FWD和BWD后，算得一份梯度G  
· 每块计算GPU将自己的梯度push给梯度收集GPU，做聚合操作。这里的聚合操作一般指**梯度累加**  
· 梯度收集GPU聚合完毕后，计算GPU从它那pull下完整的梯度效果，用于更新模型参数W，更新完毕后，计算GPU上的模型参数应该保持一致  

  实现DP的一种经典编程框架叫做“参数服务器”，在这个框架里，计算GPU成为Worker，梯度聚合GPU称为Server。在实际应用中，为了尽量减少通讯量，一般可选择一个Worker同时作为Server，比如可把梯度全发到GPU0上做聚合，这里需要说明两点：  
  · 一个worker或者server下可以不止一块GPU  
  · Server可以只做梯度聚合，也可以梯度聚合+全量参数更新一起做  
### 1.2 存在问题：通讯瓶颈和梯度异步更新
实战中存在两个问题：  
· **存储开销大**：每个GPU上都存了一份完整的模型，造成冗余。  
· **通讯开销大**：Server需要和每一个worker进行梯度传输，当Server和worker不在一台机器上时，Server的带宽将会成为整个系统的计算效率瓶颈  
当Server在搬运数据，计算梯度时，worker闲置，针对这种情况，提出了梯度异步更新策略：
**梯度异步更新**  
计算顺序如下：  
· 在第n轮计算中，该worker正常计算梯度，并向server发送push&pull梯度请求。  
· 但是，该worker不会实际等到把聚合梯度拿回来，更新完参数W后再做计算，而是直接拿旧的W，吃新的数据，继续第11轮的计算，这样就保证在通讯的时间里，worker也在马不停蹄的计算，提升计算通讯比。  
· 当然，异步也不能只计算梯度，不更新权重，那模型无法收敛，可以存在**延迟**，比如延迟为1的异步更新就是当开始第12轮对的计算时，必须保证W已经用第10、11轮的梯度做完2次更新了。  
总结一下，异步对一个worker来说，只是等于W不变，batch的数量增加了而已，在SGD下，会减慢模型的整体收敛速度，异步的整体思想是，比起让Worker闲着，倒不如让它多吃点数据，虽然反馈延迟了，但只要它在干活在学习就行。
## 二、分布式数据并行（DDP）
