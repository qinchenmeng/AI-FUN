# 数据并行
## 一、数据并行（DP）
### 1.1 整体架构
<div align=center>
  <img src="https://github.com/user-attachments/assets/d8e586fc-baa0-4438-a224-8e750a32696b" width="800" />
</div>

一个经典数据并行的过程如下：  
· 若干块计算GPU，如上图中GPU0-GPU2，1块梯度收集GPU，如图中AllReduce操作所在GPU  
· 在每块计算GPU上都拷贝一份完整的模型参数  
· 把一份数据X【例如一个batch】均匀分给不同的计算GPU  
· 每块计算GPU做一轮FWD和BWD后，算得一份梯度G  
· 每块计算GPU将自己的梯度push给梯度收集GPU，做聚合操作。这里的聚合操作一般指**梯度累加**  
· 梯度收集GPU聚合完毕后，计算GPU从它那pull下完整的梯度效果，用于更新模型参数W，更新完毕后，计算GPU上的模型参数应该保持一致  

  实现DP的一种经典编程框架叫做“参数服务器”，在这个框架里，计算GPU成为Worker，梯度聚合GPU称为Server。在实际应用中，为了尽量减少通讯量，一般可选择一个Worker同时作为Server，比如可把梯度全发到GPU0上做聚合，这里需要说明两点：  
  · 一个worker或者server下可以不止一块GPU  
  · Server可以只做梯度聚合，也可以梯度聚合+全量参数更新一起做  
### 1.2 存在问题：通讯瓶颈和梯度异步更新
