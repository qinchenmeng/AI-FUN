论文地址：https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

# 1、Transformer介绍

Transformer的提出彻底改变了自然语言处理和深度学习领域，它克服了RNN和CNN在处理长序列数据时的局限性，实现了更高效的并行计算和最强的建模能力  

Transormer的核心创新在于**自注意力机制**和**前馈神经网络**，完全摒弃了RNN结构，使得模型能够全局建模序列数据  

它的提出带来了NLP领域的BERT、GPT、T5和CV领域的ViT、Swin Transformer以及后续的多模态大模型

# 2、模型整体结构

Transformer由编码器和解码器组成，每层包含相同的层级结构  
<div align=center>
  <img src="https://github.com/user-attachments/assets/80fb98d0-ff3c-4189-ae85-252294d930ba" width="500" />
</div>

对于编码器来说包含了多头自注意力机制、前馈神经网络以及残差连接与层归一化；对于解码器，包含两个多头自注意力机制、一个前馈神经网络和三个残差连接与层归一化

# 3、Embedding层和位置编码  
1.Embedding层  

Embedding层用于将离散的token（如单词、短语、字符）的token映射到连续的高维空间。  

1）首先将原始的单词切分为token,获得其索引。不同的语言不同处理方式，对于英文，直接按照空格拆分即可得到对应token，而对于中文，因为没有空格，，需要一些工具来对句子进行拆分。具体来说首先对一句中文句子如【“我是一个中国人”】通过分词工具（如BPE/SentencePiece/AutoTokenizer）将句子拆分为token，如【'我', '是', '中国', '人'】，（这里不一定是中文也可能是通过其他编码手段处理成其他语言），随后通过tokenizer.json得到相应的索引如【21，34，28，90】    

2）将这些索引如【21，34，28，90】，经过one_hot处理后，通过embedding层即可得到一个4*512的向量（假设embedding隐藏层维度为512），即Embeeding层的基本用法，训练LLM时属于可训练模块  
2.Positional Embedding  

  传统卷积具备局部感知能力，卷积核可以在窗口内感知，如果目标是一个维度为1d的序列，那么一个大小为3的卷积核在进行卷积操作的时候就类似于一个时间窗口不断向前滑动。RNN由于他的模型结构，具备时间感知能力。而Transformer不具备这些能力，需要显示的提供位置信息，让模型知道不同token在序列中的位置。在论文中采取正余弦交替的方法进行位置编码的潜入。  
  PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)   $$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$$
 
