论文地址：https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

# 1、Transformer介绍

Transformer的提出彻底改变了自然语言处理和深度学习领域，它克服了RNN和CNN在处理长序列数据时的局限性，实现了更高效的并行计算和最强的建模能力  

Transormer的核心创新在于**自注意力机制**和**前馈神经网络**，完全摒弃了RNN结构，使得模型能够全局建模序列数据  

它的提出带来了NLP领域的BERT、GPT、T5和CV领域的ViT、Swin Transformer以及后续的多模态大模型

# 2、模型整体结构

Transformer由编码器和解码器组成，每层包含相同的层级结构  
<div align=center>
  <img src="https://github.com/user-attachments/assets/80fb98d0-ff3c-4189-ae85-252294d930ba" width="500" />
</div>

对于编码器来说包含了多头自注意力机制、前馈神经网络以及残差连接与层归一化；对于解码器，包含两个多头自注意力机制、一个前馈神经网络和三个残差连接与层归一化

# 3、Embedding层和位置编码  
**1.Embedding层**  

Embedding层用于将离散的token（如单词、短语、字符）的token映射到连续的高维空间。  

1）首先将原始的单词切分为token,获得其索引。不同的语言不同处理方式，对于英文，直接按照空格拆分即可得到对应token，而对于中文，因为没有空格，，需要一些工具来对句子进行拆分。具体来说首先对一句中文句子如【“我是一个中国人”】通过分词工具（如BPE/SentencePiece/AutoTokenizer）将句子拆分为token，如【'我', '是', '中国', '人'】，（这里不一定是中文也可能是通过其他编码手段处理成其他语言），随后通过tokenizer.json得到相应的索引如【21，34，28，90】    

2）将这些索引如【21，34，28，90】，经过one_hot处理后，通过embedding层即可得到一个4*512的向量（假设embedding隐藏层维度为512），即Embeeding层的基本用法，训练LLM时属于可训练模块  

**2.Positional Embedding**  

传统卷积具备局部感知能力，卷积核可以在窗口内感知，如果目标是一个维度为1d的序列，那么一个大小为3的卷积核在进行卷积操作的时候就类似于一个时间窗口不断向前滑动。RNN由于他的模型结构，具备时间感知能力。而Transformer不具备这些能力，需要显示的提供位置信息，让模型知道不同token在序列中的位置。在论文中采取正余弦交替的方法进行位置编码的潜入。  
<center>

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right),PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
$$

</center>

为什么采取这种位置编码？一步一步看，首先采取10000^(2i/d)的主要原因是**让低维部分变化快**，**高维部分变化慢**。  
举例对于pos=2时，即第2个单词，维度假设为4，则根据计算得到PE(2)=[0.909,−0.416,0.02,0.9998]，而PE(3)根据计算可知[0.141,0.99,0.03,0.9995]，我们可以发现，低维的部分变化很快，这意味着低维度的编码是细粒度的、局部敏感的，能够捕捉到相邻位置之间的变化，高维部分变化很慢，是粗粒度的，全局敏感的，能够编码较长距离的位置信息，这样可以捕捉全局信息。 
同时根据PE[2]和POS[3]的编码可以发现，相邻位置的编码具有平滑的数值变化，并且两个位置的点积只与k相关，即PE(POS+k)*PE(POS) = sin(A)sin(B)+COS(A)COS(B) = COS(A-B) = COS(wi*K),其中A=wi(pos+k) B = wi(pos),从而点积的结果只依赖于相对位置k，而与绝对位置POS无关。
<pre> 三角函数知识：  
1、周期：  
对于标准形式的正弦和余弦函数，基本周期为2π，即y=sin(x)当x增加到x+2π时，函数的值会重复  
对于一般形式的正弦函数：y=Asin(Bx+C)+D，其中周期T与B相关，T=2π/B，B控制频率，频率越大，周期越小，函数变化越快  
2、频率：  
频率指的是三角函数在单位时间内完成周期的次数，表示的是周期性波动的快慢程度，通常以赫兹Hz表示，即每秒完成的周期数  
频率f与周期T的关系为f=1/T，对于一般的正弦函数，B的大小决定了函数的频率，具体来说f = |B|/2π，当B增大时，频率f增大，波动速读变快，当B减小时，频率f变小，波动速读变慢</pre>  
对公式的理解：  
（1）随位置的变化：  
固定维度i，位置pos的变化将影响PE的值，即正弦和余弦函数将随着位置pos呈现出周期性变化，使得模型可以感知相对位置上的相似性。  
（2）随维度的变化：
固定位置pos，随着维度i的增加，正弦和余弦函数的频率会降低，周期会变长。所以较低维度的周期比较短，从而变化明显，即便是相邻的维度，位置编码的差异也会很大，这可以捕捉相邻维度的差异。相反，较高维度具有较长的周期，在较大位置范围内才能完成一个周期，对小的位置变化不明显，这能够帮助模型感知全局位置关系。  
举例：pos[11] = [1,余弦,500，余弦，200，余弦...0.98，余弦，0.99，余弦] 而pos[12]可能是[100,余弦,50，余弦，20，余弦...0.97，余弦，0.98，余弦]  
具体可以见下图示例。
<div align=center>
  <img src="https://github.com/user-attachments/assets/7fba85ee-7aad-4b3a-8201-414906f3e8e5" width="500" />
</div>
（3）远程衰减  
正余弦位置编码具备远程衰减特性，即对于相同的词向量，如果位置近，则内积分数高，随着相对位置的增加，内积分数震荡衰减。  

# 4、Encoder  
主要包含两个部分：多头自注意力机制和前馈神经网络  
**1、多头自注意力机制**  
多头自注意力机制的基本思想是将自注意力机制分成多个“头”（heads）并行计算。每个头可以关注序列中不同的关系或子空间，最后将所有头的输出拼接在一起，进行进一步的处理。  
子空间的概念：  
在Transformer中，每个头都有自己的Q、K、V矩阵，它们是通过不同的线性变换从原始输入中得来的。因此，每个头所学习到的表示可以看作是原始序列空间的一个“子空间”。  
  
举个例子，假设我们有一个包含词汇的输入序列：  
在第一个头中，可能会更多地关注句子中的语法关系（例如，主语和谓语之间的关系）。  
在第二个头中，可能会关注句子中的语义关系（例如，名词和动词之间的语义关联）。  
在第三个头中，可能会捕捉句子中的长距离依赖关系（例如，跨越较远位置的单词之间的关系）。  
因此，每个头的“视角”是不同的，它们捕捉到的是不同的信息子集（即不同的“子空间”）。最终，所有头的结果会被拼接并通过一个线性层融合，生成综合的表示。

多头自注意力机制的整体流程：    
（1）生成查询、键和值：每个Token会生成自己的查询向量Q、键向量K和值向量V。（输入向量X，乘以三个线性变化矩阵）  
（2）计算注意力权重：当前Token的查询向量Q与所有其他Token的键向量K进行点积，得到一个相关性分数矩阵。该矩阵通过softmax归一化，转化为注意力权重  
（3）加权求和值向量：利用得到的注意力权重对所有Token的值向量V进行加权求和。这个结果就是当前Token的最终表示，融合了它与其他Token的相关信息。  
<center>

$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V
$$

</center>

点积乘以缩放因子是为了控制 softmax 的输入值范围，避免梯度消失，让训练更加稳定和高效，为什么是根号dk，是因为方差为dk，这样做使方差为1。  
如何理解Q,K,V？  
查询向量Q:在计算注意力权重时，查询向量会与键向量进行点积计算，以度量当前Token与其他Token之间的相关性（即相似度）  
键向量K:键向量与查询向量点积，以计算当前Token和其他Token之间的匹配度，决定了注意力权重的大小。匹配度越高，表示当前Token对该Token的关注度越高。  
值向量V:在得到注意力权重（基于Q和K的点积计算）之后，权重将作用在值向量V上，通过加权求和的方式生成每个Token的输出表示。  

**2、前馈神经网络**    
FFN其实就是一个很简单的两层 MLP（多层感知机） 
FFN的作用  
（1）增强特征表达能力（引入非线性）：注意力机制本质上是线性的，FFN 引入非线性变换（ReLU、GELU），让模型更有表达力，能更好地拟合复杂关系。  
（2）提高维度，帮助模型“抽象信息”：FFN 的中间层通常是原始维度的 4 倍（比如 768 → 3072 → 768），这个升维→降维过程类似于“信息压缩+抽象”，可以让模型在更高维空间中做出更复杂的特征变换。  
（3）对每个位置进行单独加工：注意力是看别人的（每个 token 看其他 token），而 FFN 是“内省式”的 —— 每个 token 单独加工自己，相当于进一步细化每个词的表示。
