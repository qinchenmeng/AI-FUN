论文地址：https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

# 1、Transformer介绍

Transformer的提出彻底改变了自然语言处理和深度学习领域，它克服了RNN和CNN在处理长序列数据时的局限性，实现了更高效的并行计算和最强的建模能力  

Transormer的核心创新在于**自注意力机制**和**前馈神经网络**，完全摒弃了RNN结构，使得模型能够全局建模序列数据  

它的提出带来了NLP领域的BERT、GPT、T5和CV领域的ViT、Swin Transformer以及后续的多模态大模型

# 2、模型整体结构

Transformer由编码器和解码器组成，每层包含相同的层级结构  
<div align=center>
  <img src="https://github.com/user-attachments/assets/80fb98d0-ff3c-4189-ae85-252294d930ba" width="500" />
</div>

对于编码器来说包含了多头自注意力机制、前馈神经网络以及残差连接与层归一化；对于解码器，包含两个多头自注意力机制、一个前馈神经网络和三个残差连接与层归一化

# 3、Embedding层和位置编码  
1.Embedding层  

Embedding层用于将离散的token（如单词、短语、字符）的token映射到连续的高维空间。  

1）首先将原始的单词切分为token,获得其索引。不同的语言不同处理方式，对于英文，直接按照空格拆分即可得到对应token，而对于中文，因为没有空格，，需要一些工具来对句子进行拆分。具体来说首先对一句中文句子如【“我是一个中国人”】通过分词工具（如BPE/SentencePiece/AutoTokenizer）将句子拆分为token，如【'我', '是', '中国', '人'】，（这里不一定是中文也可能是通过其他编码手段处理成其他语言），随后通过tokenizer.json得到相应的索引如【21，34，28，90】    

2）将这些索引如【21，34，28，90】，经过one_hot处理后，通过embedding层即可得到一个4*512的向量（假设embedding隐藏层维度为512），即Embeeding层的基本用法，训练LLM时属于可训练模块
