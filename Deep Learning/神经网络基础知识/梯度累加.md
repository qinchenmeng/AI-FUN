# 梯度累加
## Gradient Accumulation
定义：梯度累加，就是将多次计算的梯度值进行累加，然后进行一次性的参数更新  
使用场景：常用于单卡训练显存不足时，将其分为多个小的mini-batch，每个step送入一个mini-batch获得梯度，但是不马上更新参数，而是将多次获得的梯度进行累加后再更新参数，以达到模拟单次使用大的batch训练的目的

## 代码
```
for i , (context,label) in enumerate(train_loader):
    preds = model(context) # 前向传播
    loss = loss_function(preds,label)  # 计算loss
    loss = loss / accumulation_steps   # 核心步骤
    loss.backward() # 计算梯度，但是要知道，backward只是计算了梯度，并没有将参数更新，在这里用于计算，随后累加梯度
    if (i+1) % accumulation_steps == 0:  # 只有当累积到accumulation_steps整数倍的时候
        optimizer.step()  # 参数才会更新
        optimizer.zero_grad()  # 清除之前累积的梯度
```
